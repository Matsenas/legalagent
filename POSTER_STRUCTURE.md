# A0 POSTER STRUCTURE: Czech Legal Information Retrieval with Reranking

## Layout Overview

```
┌────────────────────────────────────────┐
│  TITLE + AUTHORS + AFFILIATION         │
├────────────────────────────────────────┤
│  ABSTRACT (2-3 sentences)              │
├────────────────────────────────────────┤
│  [Column 1]        │  [Column 2]       │
│                    │                   │
│  1. MOTIVATION     │  4. METHODS       │
│     & PROBLEM      │     COMPARISON    │
│                    │                   │
│  2. ARCHITECTURE   │  5. RESULTS       │
│                    │     & EVAL        │
│                    │                   │
│ 3. ALGORITHMS      │  6. INSIGHTS      │
│    & COMPLEXITY    │                   │
└────────────────────────────────────────┘
```

---

## DETAILED SECTIONS

### 1. ABSTRACT (Top banner, 50-75 words)

> "We investigate three retrieval methods for Czech legal Q&A: baseline ANN search, FlashRank cross-encoder reranking, and a novel PageRank-inspired reranker. We evaluate accuracy (cosine similarity to ground truth) and latency on 100 legal queries. FlashRank achieves best accuracy (0.625 best-of-3) with minimal overhead (+32ms), while our PageRank method provides diversity at higher computational cost (+327ms)."

---

### 2. MOTIVATION & PROBLEM (Column 1, top)

**Key Points:**
- **Domain Challenge**: Legal information retrieval requires high precision—incorrect advice has serious consequences
- **Language Specificity**: Czech legal terminology needs multilingual embedding support
- **Scale**: 52MB dataset with complex legal documents (titles, questions, answers)
- **Research Question**: How do different reranking strategies affect the accuracy/latency tradeoff for legal retrieval?

**Visual**: Show example legal query → multiple similar cases → need for accurate ranking

---

### 3. ARCHITECTURE (Column 1, bottom)

**System Pipeline Diagram:**
```
Query → Embedding → Pinecone HNSW → Top-k Docs → Reranker → Top-3 → LLM → Answer
         (384d)      (O(log N))      (k=15)      (Optional)
```

**Key Components:**
- **Embeddings**: `paraphrase-multilingual-MiniLM-L12-v2` (384d, Czech support)
- **Vector DB**: Pinecone with HNSW indexing
- **Backend**: LangGraph state machine with query classification
- **Evaluation**: 100-question test set with ground truth exclusion

---

### 4. ALGORITHMS & COMPLEXITY (Column 2, top) ⚠️ CRITICAL FOR ALGORITHMICS

**Method 1: Baseline (ANN Only)**
- Complexity: **O(log N)** query time (HNSW)
- Direct Pinecone similarity search, k=3

**Method 2: FlashRank Reranking**
- Retrieval: O(log N)
- Reranking: **O(k × m²)** where k=15, m=256 tokens
- Cross-encoder (TinyBERT-2L) joint query-document encoding
- Total: **O(log N + k × m²)**

**Method 3: PageRank Reranking (Novel)**
- Retrieval: O(log N)
- Graph Construction: **O(k² × d)** where d=384 dims
- Power Iteration: **O(I × k²)** where I=10 iterations
- Total: **O(log N + k² × d + I × k²)**

**Visual**: Complexity comparison chart or table showing theoretical vs. actual runtime

**PageRank Algorithm Box:**
```
1. Build similarity graph (threshold=0.3)
2. Create adjacency matrix A (row-normalized)
3. Power iteration: r^(t+1) = α·A·r^(t) + (1-α)·v
4. Combine: score = 0.5·retrieval + 0.5·pagerank
```

---

### 5. METHODS COMPARISON (Column 2, bottom)

**Visual Table:**
```
┌────────────┬─────────────┬──────────────┬─────────┬──────────┐
│ Method     │ Top-1 Sim   │ Best-3 Sim   │ Latency │ Rerank   │
├────────────┼─────────────┼──────────────┼─────────┼──────────┤
│ Baseline   │ 0.579       │ 0.616        │ 221 ms  │ 0 ms     │
│ FlashRank  │ 0.576       │ 0.625 ✓      │ 253 ms  │ 39 ms    │
│ PageRank   │ 0.579       │ 0.612        │ 541 ms  │ 327 ms   │
└────────────┴─────────────┴──────────────┴─────────┴──────────┘
```

**Key Insight Visual**: Accuracy vs. Latency scatter plot

---

### 6. RESULTS & EVALUATION (Column 3, top)

**Evaluation Methodology:**
- **Dataset**: 100 sampled questions from Czech legal Q&A corpus
- **Metric**: Cosine similarity between retrieved answers and ground truth
- **Test Set Exclusion**: Ground truth documents removed from retrieval pool
- **Reproducibility**: Fixed random seed, comprehensive JSON logging

**Bar Charts** (from evaluation/results/):
- Top-1 similarity comparison
- Best-of-3 similarity comparison
- Latency breakdown (retrieval vs. reranking)

**Include actual chart images generated by `generate_charts.py`**

---

### 7. INSIGHTS & CONCLUSIONS (Column 3, bottom)

**Critical Points for Legal Retrieval:**

1. **Accuracy vs. Speed Tradeoff**
   - FlashRank: Best accuracy with acceptable latency (+14% overhead)
   - Baseline: Fast but 1.4% lower best-of-3 accuracy
   - PageRank: Similar accuracy, 2.4× slower (graph construction bottleneck)

2. **Why Accuracy Matters in Legal Domain**
   - Incorrect legal advice has serious consequences
   - Best-of-3 metric crucial: users typically review multiple results
   - FlashRank's cross-encoder captures subtle legal distinctions

3. **Algorithm Complexity Insights**
   - HNSW scales to millions: O(log N) enables production deployment
   - FlashRank's O(k × m²) is tractable for small k
   - PageRank's O(k² × d) becomes expensive for high-dim embeddings

4. **Recommendations**
   - **Production**: Use FlashRank for best accuracy/latency balance
   - **Large-scale**: Baseline sufficient if milliseconds matter
   - **Future Work**: Optimize PageRank with approximate nearest neighbors in graph construction

**Visual**: Decision tree or flowchart for method selection based on requirements

---

## MUST-COVER POINTS FOR ALGORITHMICS COURSE ⚠️

1. **Complexity Analysis** (formal big-O notation with variables defined)
2. **Algorithm Descriptions** (pseudocode or clear step-by-step for PageRank)
3. **Data Structures** (HNSW graph, adjacency matrix, embeddings)
4. **Tradeoffs** (time vs. accuracy, space vs. speed)
5. **Scalability** (how each method scales with N, k, d)
6. **Empirical Validation** (theoretical complexity vs. measured latency)
7. **Graph Algorithms** (PageRank as power iteration on stochastic matrix)

---

## VISUAL DESIGN TIPS

- **Whitespace**: Don't overcrowd—leave breathing room
- **Flow**: Guide eye from problem → solution → evaluation → insights
- **Highlight**: Box the PageRank algorithm and complexity formulas

---

## SUMMARY

The poster emphasizes:

- **Theoretical foundations**: Big-O complexity analysis for all three methods
- **Practical implementation**: Real evaluation on 100 Czech legal queries
- **Novel contribution**: PageRank-inspired reranking algorithm
- **Domain relevance**: Legal information retrieval challenges and requirements
- **Engineering tradeoffs**: Accuracy, latency, and scalability considerations
